{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3076aa9d",
   "metadata": {},
   "source": [
    "# Polynomial regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2687f",
   "metadata": {},
   "source": [
    "**What is Polynomial Regression**\n",
    "\n",
    "* Polynomial regression is an extension of linear regression: instead of fitting a straight line, you fit a polynomial of degree *n* to capture non‑linear relationships between an independent variable $x$ and a dependent variable $y$. ([GeeksforGeeks][1])\n",
    "* The general form is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\varepsilon\n",
    "$$\n",
    "\n",
    "where $\\beta_i$ are coefficients and $\\varepsilon$ is an error term. ([GeeksforGeeks][1])\n",
    "\n",
    "**Issues, Trade‑offs & Tips**\n",
    "\n",
    "* **Overfitting vs Underfitting**: As degree increases, the model may fit training data very well but perform poorly on unseen data (overfit). Low degree may underfit, fail to capture structure. Need to balance. ([GeeksforGeeks][1])\n",
    "* **Bias‑Variance Trade‑off**: Choosing the polynomial degree is a way of navigating that trade‑off: lower degree ≈ high bias, lower variance; higher degree ≈ low bias, high variance. ([GeeksforGeeks][1])\n",
    "* **Sensitivity to outliers**: Because polynomial features amplify small differences (especially with high degree), outliers can pull the fit significantly. ([GeeksforGeeks][1])\n",
    "\n",
    "[1]: https://www.geeksforgeeks.org/machine-learning/python-implementation-of-polynomial-regression/ \"Implementation of Polynomial Regression - GeeksforGeeks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2d7d0",
   "metadata": {},
   "source": [
    "### Implementation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba360fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utils import save_metrics, save_predictions\n",
    "\n",
    "def fit_polynomial_models(x_train, Y_train, x_test, y_test, degree_rang = range(2,10)):\n",
    "    num_realizations = Y_train.shape[1]\n",
    "\n",
    "    # Initialize result storage\n",
    "    Y_pred = np.zeros((len(x_test), num_realizations))\n",
    "    MSE_train = np.zeros(num_realizations)\n",
    "    MSE_test = np.zeros(num_realizations)\n",
    "    bias_per_model = np.zeros((len(x_test), num_realizations))   # pointwise error\n",
    "    best_degree = 2\n",
    "\n",
    "    # Loop over each realization\n",
    "    for i in range(num_realizations):\n",
    "        y_i = Y_train[:, i]  # Current noisy realization\n",
    "        best_mse_test = np.inf  # Initialize with a high test MSE\n",
    "\n",
    "        # Try all polynomial degrees from min to max\n",
    "        for d in degree_rang:\n",
    "            # Fit polynomial of degree d\n",
    "            coeffs = np.polyfit(x_train, y_i, d)\n",
    "\n",
    "            # Make predictions\n",
    "            y_train_pred = np.polyval(coeffs, x_train)\n",
    "            y_test_pred = np.polyval(coeffs, x_test)\n",
    "\n",
    "            # Compute MSE for training and test\n",
    "            mse_tr = mean_squared_error(y_i, y_train_pred)\n",
    "            mse_te = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "            # Keep the model with lowest test error\n",
    "            if mse_te < best_mse_test:\n",
    "                best_mse_test = mse_te\n",
    "                best_mse_train = mse_tr\n",
    "                best_pred = y_test_pred\n",
    "                if (i == 1): best_degree = d\n",
    "\n",
    "        # Store results for current realization\n",
    "        Y_pred[:, i] = best_pred\n",
    "        MSE_train[i] = best_mse_train\n",
    "        MSE_test[i] = best_mse_test\n",
    "        bias_per_model[:, i] = np.abs(best_pred - y_test)\n",
    "\n",
    "    # Final metrics\n",
    "    Bias = np.mean(bias_per_model)\n",
    "    Variance = np.var(Y_pred, axis=1).mean()  # Mean of variances across all test points\n",
    "\n",
    "    roundN = 8\n",
    "    metrics =  { # Dictionary of metrics\n",
    "        \"Name\": f\"Poly deg= {best_degree}\",\n",
    "        \"MSE_train\": round(float(np.mean(MSE_train)), roundN),\n",
    "        \"MSE_test\": round(float(np.mean(MSE_test)), roundN),\n",
    "        \"Bias\": round(float(Bias), roundN),\n",
    "        \"Variance\": round(float(Variance), roundN),\n",
    "    }\n",
    "\n",
    "    save_metrics(\"polynomial\", metrics)\n",
    "    save_predictions(\"polynomial\", Y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
