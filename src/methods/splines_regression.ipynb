{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844b2ac0",
   "metadata": {},
   "source": [
    "# Splines regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98cf31",
   "metadata": {},
   "source": [
    "This section describes different spline-based methods for flexible regression that allow modeling non‐linear relationships by partitioning the input space and imposing smoothness constraints.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "* **Knots**: Points $z_i$ that partition the domain into intervals; in each interval a polynomial is fit. ([rubenfcasal.github.io][1])\n",
    "* **Piecewise regression**: Fit separate polynomials in each interval; naive version can lead to discontinuities at the knots unless continuity/differentiability constraints are imposed. ([rubenfcasal.github.io][1])\n",
    "\n",
    "#### 4.2.1 Regression Splines\n",
    "\n",
    "* In each interval, one fits a polynomial of degree $d$, with constraints so the function and its derivatives up to order $d-1$ are continuous across knots. ([rubenfcasal.github.io][1])\n",
    "* A popular choice is cubic splines ($d=3$). One common basis is the truncated power basis:\n",
    "\n",
    "  $$\n",
    "    1, x, x^2, \\dots, x^d, (x - z_1)_+^d, \\dots, (x - z_k)_+^d\n",
    "  $$\n",
    "\n",
    "  where $(x - z)_+ = \\max(0, x - z)$. ([rubenfcasal.github.io][1])\n",
    "* Another basis is the **B-spline** basis, which tends to have better numerical properties. In R, this is implemented via `bs()` in the `splines` package. ([rubenfcasal.github.io][1])\n",
    "* **Natural splines** add further constraints: the fit is linear in the tails (outside the outermost knots), which improves stability at the boundaries. In R: `ns()`. ([rubenfcasal.github.io][1])\n",
    "* Choice of number of knots (or equivalently degrees of freedom) is crucial. Can use equally spaced knots, quantile‐based, or more knots where the function varies more. Cross‐validation is often used to select this. ([rubenfcasal.github.io][1])\n",
    "\n",
    "#### 4.2.2 Smoothing Splines\n",
    "\n",
    "* Instead of preset knots, smoothing splines choose a smooth function $s(x)$ (twice differentiable) that minimizes\n",
    "\n",
    "  $$\n",
    "    \\sum_i (y_i - s(x_i))^2 + \\lambda \\int [s''(x)]^2 \\, dx\n",
    "  $$\n",
    "\n",
    "  where $\\lambda\\ge0$ is a smoothing parameter—small $\\lambda$ yields a very wiggly fit (closer to interpolating the data); large $\\lambda$ forces smoother behavior (approaching a linear fit as $\\lambda \\to \\infty$). ([rubenfcasal.github.io][1])\n",
    "* For univariate $x$, the solution is a natural cubic spline with knots at every observed $x_i$, with smoothness determined by $\\lambda$. ([rubenfcasal.github.io][1])\n",
    "* For multivariate inputs, there are generalizations such as thin‐plate splines. ([rubenfcasal.github.io][1])\n",
    "* Selection of $\\lambda$ can be done via leave‐one‐out cross‐validation (CV) or generalized cross validation (GCV). Also, the effective degrees of freedom (trace of the smoother matrix) serves as a measure of complexity. ([rubenfcasal.github.io][1])\n",
    "\n",
    "#### 4.2.3 Penalized Splines\n",
    "\n",
    "* These combine features of regression splines and smoothing splines. Use a relatively small number of knots, but impose a penalty on the coefficients (e.g. penalize differences between adjacent coefficients) to control wiggliness. Such models are often called **low‐rank smoothers**. ([rubenfcasal.github.io][1])\n",
    "* An example: **P‑splines** (Eilers & Marx, 1996) use B‑spline basis + penalty on squared differences of coefficients. ([rubenfcasal.github.io][1])\n",
    "* These models can be cast in the framework of mixed‐effects models, which allows leveraging tools from that setup (e.g. as in R packages `nlme`, `mgcv`). In particular, `mgcv` commonly uses penalized splines. ([rubenfcasal.github.io][1])\n",
    "\n",
    "[1]: https://rubenfcasal.github.io/aprendizaje_estadistico/splines.html \"7.2 Splines | Métodos predictivos de aprendizaje estadístico\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45418220",
   "metadata": {},
   "source": [
    "### Implementation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248b30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from utils import save_metrics, save_predictions\n",
    "\n",
    "def fit_spline_models(x_train, Y_train, x_test, y_test, n_knots_range=range(5, 15)):\n",
    "    num_models = Y_train.shape[1]\n",
    "    Y_pred = np.zeros((len(x_test), num_models))\n",
    "    MSE_train = np.zeros(num_models)\n",
    "    MSE_test = np.zeros(num_models)\n",
    "    bias_per_model = np.zeros((len(x_test), num_models))  # pointwise error\n",
    "    #kn = 5\n",
    "\n",
    "    for i in range(num_models):\n",
    "        y_i = Y_train[:, i]\n",
    "        best_mse_test = np.inf\n",
    "        best_pred = None\n",
    "        best_mse_train = None\n",
    "\n",
    "        for n_knots in n_knots_range:\n",
    "            # Create spline + regression pipeline\n",
    "            pipeline = make_pipeline(\n",
    "                SplineTransformer(n_knots=n_knots, degree=3, include_bias=False),\n",
    "                LinearRegression()\n",
    "            )\n",
    "\n",
    "            # Fit the model\n",
    "            pipeline.fit(x_train.reshape(-1, 1), y_i)\n",
    "\n",
    "            # Predict\n",
    "            y_pred_train = pipeline.predict(x_train.reshape(-1, 1))\n",
    "            y_pred_test = pipeline.predict(x_test.reshape(-1, 1))\n",
    "            # Calculate MSEs\n",
    "            mse_train = mean_squared_error(y_i, y_pred_train)\n",
    "            mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "            if mse_test < best_mse_test:\n",
    "                best_mse_test = mse_test\n",
    "                best_mse_train = mse_train\n",
    "                best_pred = y_pred_test\n",
    "                #if (i==1): kn = n_knots\n",
    "\n",
    "        # Save best model results for curve i\n",
    "        Y_pred[:, i] = best_pred\n",
    "        MSE_train[i] = best_mse_train\n",
    "        MSE_test[i] = best_mse_test\n",
    "        bias_per_model[:, i] = np.abs(best_pred - y_test)\n",
    "\n",
    "    # Calculate bias (average mean error)\n",
    "    Bias = np.mean(bias_per_model)\n",
    "    # Calculate variance (average variance per point)\n",
    "    Variance = np.mean(np.var(Y_pred, axis=1))\n",
    "\n",
    "    roundN = 8\n",
    "    metrics= {\n",
    "        \"Name\": f\"Splines deg=3\",\n",
    "        \"MSE_train\": round(float(np.mean(MSE_train)), roundN),\n",
    "        \"MSE_test\": round(float(np.mean(MSE_test)), roundN),\n",
    "        \"Bias\": round(float(Bias), roundN),\n",
    "        \"Variance\": round(float(Variance), roundN),\n",
    "    }\n",
    "\n",
    "    save_metrics(\"splines\", metrics)\n",
    "    save_predictions(\"splines\", Y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
