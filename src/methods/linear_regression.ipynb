{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598c91ad",
   "metadata": {},
   "source": [
    "# Linear regression implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9f7cd",
   "metadata": {},
   "source": [
    "**Definition & Purpose**\n",
    "\n",
    "* Linear Regression is a **supervised learning** algorithm used to model the relationship between one or more independent variables (inputs) and a dependent variable (output) by fitting a linear equation. ([GeeksforGeeks][1])\n",
    "* It assumes the output changes at a constant rate with respect to each input. ([GeeksforGeeks][1])\n",
    "\n",
    "<a href=\"https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/\" target=\"_blank\">\n",
    "    <figure>\n",
    "      <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png\" alt=\"GeeksforGeeks\" width=\"800\">\n",
    "      <figcaption>Linear regression. GeeksforGeeks.</figcaption>\n",
    "    </figure>\n",
    "</a> \n",
    "\n",
    "([GeeksforGeeks][1])\n",
    "\n",
    "**Equation / Hypothesis Function**\n",
    "\n",
    "* For **simple linear regression** (one feature):\n",
    "  $$\n",
    "  \\hat{y} = \\theta_0 + \\theta_1 x\n",
    "  $$\n",
    "* For **multiple linear regression** (multiple features):\n",
    "  $$\n",
    "  \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n\n",
    "  $$\n",
    "  ([GeeksforGeeks][1])\n",
    "\n",
    "**Finding the Best Fit (Optimization)**\n",
    "\n",
    "* Use the **Least Squares Method** to minimize the sum of squared residuals:\n",
    "  $$\n",
    "  \\sum (y_i - \\hat y_i)^2\n",
    "  $$\n",
    "  This yields optimal parameter values (βs). ([GeeksforGeeks][1])\n",
    "* **Gradient Descent** is an iterative optimization method often used to update parameters (βs) to reduce the loss. ([GeeksforGeeks][1])\n",
    "\n",
    "**Cost / Loss Function**\n",
    "\n",
    "* **Mean Squared Error (MSE)** is commonly used:\n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum (y_i - \\hat y_i)^2\n",
    "  $$\n",
    "  Lower MSE → better fit. ([GeeksforGeeks][1])\n",
    "\n",
    "**Assumptions of Linear Regression**\n",
    "\n",
    "1. Linearity: The relationship between inputs and output is linear. ([GeeksforGeeks][1])\n",
    "2. Independence of errors (residuals). ([GeeksforGeeks][1])\n",
    "3. Homoscedasticity: Constant variance of errors across all levels of input. ([GeeksforGeeks][1])\n",
    "4. Normality of errors (residuals). ([GeeksforGeeks][1])\n",
    "5. No multicollinearity (for multiple regression): inputs not highly correlated. ([GeeksforGeeks][1])\n",
    "6. No autocorrelation (especially for time‑series data). ([GeeksforGeeks][1])\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "* **Mean Squared Error (MSE)**\n",
    "* **Mean Absolute Error (MAE)**: average absolute difference between predictions and actuals ([GeeksforGeeks][1])\n",
    "* **Root Mean Squared Error (RMSE)**: square root of the MSE ([GeeksforGeeks][1])\n",
    "* **Coefficient of Determination (R²)**: proportion of variance in the dependent variable explained by the model (value between 0 and 1) ([GeeksforGeeks][1])\n",
    "* **Adjusted R²**: adjusts R² by penalizing unnecessary predictors (useful in multiple regression) ([GeeksforGeeks][1])\n",
    "\n",
    "**Regularization Techniques** *(to avoid overfitting / handle multicollinearity)*\n",
    "\n",
    "* **Ridge Regression (L2 regularization)**: adds penalty proportional to squared magnitude of coefficients ([GeeksforGeeks][1])\n",
    "* **Lasso Regression (L1 regularization)**: adds penalty proportional to absolute values of coefficients (can drive some coefficients to zero) ([GeeksforGeeks][1])\n",
    "* **Elastic Net**: combines L1 and L2 penalties ([GeeksforGeeks][1])\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Simple to understand, interpret, and implement ([GeeksforGeeks][1])\n",
    "* Computationally efficient ([GeeksforGeeks][1])\n",
    "* Provides insight into the relationship between variables (coefficients have meaning) ([GeeksforGeeks][1])\n",
    "* Serves as a baseline for comparing more complex models ([GeeksforGeeks][1])\n",
    "\n",
    "**Disadvantages / Limitations**\n",
    "\n",
    "* Assumes linear relationships (so it performs poorly if the true relation is nonlinear) ([GeeksforGeeks][1])\n",
    "* Sensitive to outliers — large deviations can disproportionately influence the model ([GeeksforGeeks][1])\n",
    "* Multicollinearity among features degrades coefficient stability ([GeeksforGeeks][1])\n",
    "* May underfit when the relationship is more complex than a linear one ([GeeksforGeeks][1])\n",
    "\n",
    "**Applications**\n",
    "\n",
    "* Predicting real estate prices based on features (size, location, etc.) ([GeeksforGeeks][1])\n",
    "* Forecasting economic indicators, stock prices, etc. ([GeeksforGeeks][1])\n",
    "* Analyzing relationships in healthcare, marketing, etc.\n",
    "\n",
    "[1]: https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/ \"Linear Regression in Machine learning - GeeksforGeeks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae939a",
   "metadata": {},
   "source": [
    "### Store metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150eb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_metrics(metrics):\n",
    "    with open(\"results/metrics/linear.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0155f",
   "metadata": {},
   "source": [
    "### Implementation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd84d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_models(x_train, Y_train, x_test, y_test):\n",
    "    num_realizations = Y_train.shape[1]\n",
    "\n",
    "    # Initialize result storage\n",
    "    Y_pred = np.zeros((len(x_test), num_realizations))\n",
    "    MSE_train = np.zeros(num_realizations)\n",
    "    MSE_test = np.zeros(num_realizations)\n",
    "    bias_per_model = np.zeros((len(x_test), num_realizations)) \n",
    "\n",
    "    # Loop over each realization\n",
    "    for i in range(num_realizations):\n",
    "        y_i = Y_train[:, i]  # Current noisy realization\n",
    "\n",
    "        #Fit line: y = a0*x + a1\n",
    "        a0, a1 = np.polyfit(x_train, y_i, 1)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = a0 * x_train + a1\n",
    "        y_test_pred = a0 * x_test + a1\n",
    "\n",
    "        # Compute MSE for training and test\n",
    "        mse_tr = mean_squared_error(y_i, y_train_pred)\n",
    "        mse_te = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        Y_pred[:, i] = y_test_pred\n",
    "        MSE_train[i] = mse_tr\n",
    "        MSE_test[i] = mse_te\n",
    "        bias_per_model[:,i] = np.abs(y_test_pred - y_test)\n",
    "\n",
    "    Bias = np.mean(bias_per_model)\n",
    "\n",
    "    Variance = np.var(Y_pred, axis=1).mean()  # Mean of variances across all test points\n",
    "\n",
    "    roundN = 8\n",
    "    # Create dictionary with metrics and store its in the results/metrics\n",
    "    metrics = {\n",
    "        \"MSE_train\": round(float(np.mean(MSE_train)), roundN),\n",
    "        \"MSE_test\": round(float(np.mean(MSE_test)), roundN),\n",
    "        \"Bias\": round(float(Bias), roundN),\n",
    "        \"Variance\": round(float(Variance), roundN),\n",
    "    }\n",
    "    store_metrics(metrics)\n",
    "\n",
    "    #Store predictions for the linear models\n",
    "    np.save(\"results/predictions/linear_preds.npy\", Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
